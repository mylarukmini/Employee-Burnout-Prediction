{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mylarukmini/Employee-Burnout-Prediction/blob/main/Employee%20Burnout%20Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Employee Burnout Prediction**"
      ],
      "metadata": {
        "id": "1xVkARTyrc_l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lggS-4Nksjf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing necessary libraries**"
      ],
      "metadata": {
        "id": "jhGwruKcslDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary Libraries\n",
        "import pandas as pd #used for data manipulations and data analysis\n",
        "import numpy as np #used for arrays, it is also known as numerical python to support mathemtical operations and it supports 'n' dimentional, for scentiifc computation and arrays related concepts\n",
        "import matplotlib.pyplot as plt #used for data visualisation\n",
        "import seaborn as sns #used for data visualisation on complex datasets\n",
        "from sklearn.model_selection import train_test_split #used for training and testing of model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import pickle as pickle #this is used for storing the data like processed data and so on\n",
        "import os #used for file operations (like opening a file, reading a file etc..)"
      ],
      "metadata": {
        "id": "PkTnJddHs_Y2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Dataset**"
      ],
      "metadata": {
        "id": "CPlDGvsVuj1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#declared the variable named data and stored the data set named employee_burnour_analysis in the variable and this is done using the library\n",
        "#pandas which is used for data manipulation and data analysis\n",
        "data = pd.read_excel(\"/content/employee_burnout_analysis-AI.xlsx\")"
      ],
      "metadata": {
        "id": "FBhAuVmguuSX",
        "outputId": "07346f95-2324-4b9c-d663-018cf22b4ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/employee_burnout_analysis-AI.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c5c2a9d20a43>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#declared the variable named data and stored the data set named employee_burnour_analysis in the variable and this is done using the library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pandas which is used for data manipulation and data analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/employee_burnout_analysis-AI.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1497\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/employee_burnout_analysis-AI.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Overview**"
      ],
      "metadata": {
        "id": "zo9uQ6s_vsop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()\n",
        "#the function data.head() is used for displaying the values from the beginning of the dataset\n",
        "#the default no of values it displays is 5 without any parameter\n",
        "#if we give any parameter like 10 or any number 'x' then it displays x number of entries from the beginning of the dataset\n",
        "#the below result are the 1st 5 entries of the dataset"
      ],
      "metadata": {
        "id": "63xMrnuCvv5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()\n",
        "#the function data.tail() is used for displaying values from the end of the dataset\n",
        "#the default no of values it displays is 5 without any parameter\n",
        "#if we give any parameter like 10 or any number 'x' then it displays x number of entries from the bottom of the dataset\n",
        "#the below result are the last 5 entries of the dataset"
      ],
      "metadata": {
        "id": "F3G62u4TvxmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()\n",
        "#describe() The describe() method is used for calculating some statistical data like percentile, mean and std of the numerical values\n",
        "#of the Series or DataFrame.(like mean, mode, quartile, standard deviation)"
      ],
      "metadata": {
        "id": "ysQo5n28wgUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.nunique()\n",
        "#The nunique() method returns the number of unique values for each column."
      ],
      "metadata": {
        "id": "KX9nPaWIw1vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()\n",
        "#The info() method prints information about the DataFrame. The information contains the number of columns, column labels,\n",
        "#column data types, memory usage, range index, and the number of cells in each column (non-null values)."
      ],
      "metadata": {
        "id": "OQAhK4w3xsOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()\n",
        "#the return type of isna() is boolean so it returns true if there are null values and returns false if there are no nulll values\n",
        "#isna().sum() is used to sum all the null values present in each column and return in integer type\n",
        "#this displays the no unique values present in each column"
      ],
      "metadata": {
        "id": "LjUO9VTDx77F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()\n",
        "#this displays the no unique values present in each column"
      ],
      "metadata": {
        "id": "wQMNTyaNyPbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum().values.sum()\n",
        "#this statement gives the overall  null values in the whole dataset or dataframe\n",
        "#this does not give for each column, it gives for the whole dataset"
      ],
      "metadata": {
        "id": "OmQLTk2PydiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EDA (Exploratory Data Analysis)**"
      ],
      "metadata": {
        "id": "UnYdy_49zuIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.corr(numeric_only=True)['Burn Rate'][:-1]\n",
        "#The corr() method is used to find the pairwise correlation of all columns in the DataFrame.\n",
        "#numeric_only=True this is used to find the correlation for all the data with numeric datatype\n",
        "#all these columns are the correlation with the burnnout rate\n",
        "#like Designation  =  0.736412 is the correaltion of the Designation with the burnout rate(like how much percentage they are related or they effect each other)"
      ],
      "metadata": {
        "id": "ax-G2qBcmPHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data)\n",
        "plt.show()\n",
        "#This command creates a pairplot of the DataFrame `data`, which includes scatter plots for each pair of numeric columns\n",
        "#and histograms for each individual numeric column, and then displays the plot.\n",
        "#this give the visualisation of correlation btw the columns"
      ],
      "metadata": {
        "id": "1xTwDD4xoRTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.scatter_matrix(data, alpha=0.2, figsize=(10, 10))\n",
        "plt.show()\n",
        "#This command generates a scatter plot matrix for all pairs of columns in the DataFrame data, with a transparency level of 0.2\n",
        "#and a figure size of 10x10, and then displays the plot."
      ],
      "metadata": {
        "id": "mDmggnZ4kixf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape\n",
        "#this is used displaying the no of rows and columns present in the dataset"
      ],
      "metadata": {
        "id": "nB9GDIuM0Hc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()\n",
        "#this is used for droping the null values"
      ],
      "metadata": {
        "id": "3JJ-vTLe0Zna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "84MQkfNi0fG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes\n",
        "#this function is used for displaying the datatypes of the columns or features available"
      ],
      "metadata": {
        "id": "a5UP5Nx90kxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns='Employee ID')\n",
        "#this is used for droping the column names Employee ID\n",
        "#we dropped this column as it does not effect the burn rate"
      ],
      "metadata": {
        "id": "K3-AKNU904AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data\n",
        "#displayig the dataset"
      ],
      "metadata": {
        "id": "fKoJCdVx1hcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum().values.sum()\n",
        "#calculating the total no of null values in the given dataset"
      ],
      "metadata": {
        "id": "H7KuoaxT1i4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Min date {data['Date of Joining'].min()}\")\n",
        "print(f\"Min date {data['Date of Joining'].max()}\")\n",
        "data_month = data.copy()\n",
        "\n",
        "data_month['Date of Joining'] = data_month['Date of Joining'].astype('datetime64[ns]')\n",
        "data_month['Date of Joining'].groupby(data_month['Date of Joining'].dt.month).count().plot(kind = 'bar', xlabel = 'Month', ylabel ='Hired employees')\n",
        "\n",
        "#This code snippet prints the earliest and latest dates from the 'Date of Joining' column, converts this column to datetime format, groups\n",
        "#the data by the month of joining, counts the number of hires per month, and then plots these counts as a bar chart with 'Month' on the x-axis\n",
        "#and 'Hired employees' on the y-axis"
      ],
      "metadata": {
        "id": "HYEeFSjfVxAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "-TXI4JtfZAdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "monthly_hires = data_month['Date of Joining'].groupby(data_month['Date of Joining'].dt.month).count()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_hires.index, monthly_hires.values, marker='o')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Hired employees')\n",
        "plt.title('Monthly Hires')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#This code calculates the number of hires per month, creates a line plot with markers showing the monthly hires,\n",
        "#and then displays the plot with labeled axes, a title, and a grid for better readability."
      ],
      "metadata": {
        "id": "gLcA3SRfazHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aLql-tlDbEbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_2008 = pd.to_datetime([\"2008-01-01\"] * len(data)) #from date of joining to this particular date how many days they are working\n",
        "data[\"Days\"] = data['Date of Joining'].astype('datetime64[ns]').sub(data_2008).dt.days #\n",
        "data.Days\n",
        "#from this we can get the most senior person\n",
        "#This code calculates the number of days each employee has been working from their 'Date of Joining' until January 1, 2008, adds this as a\n",
        "#new column 'Days' to the DataFrame, and then displays the 'Days' column, allowing identification of the most senior employee."
      ],
      "metadata": {
        "id": "uItxPp22Ungn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we should select olny the numberis data before calculating the correlation\n",
        "numeric_data = data.select_dtypes(include=['number'])\n",
        "correlation = numeric_data.corr()['Burn Rate']\n",
        "print(correlation)\n",
        "\n",
        "#This code selects only the numeric columns from the DataFrame data, calculates the correlation of each of these numeric columns with the\n",
        "#'Burn Rate' column, and prints the resulting correlation values."
      ],
      "metadata": {
        "id": "2JVePTg2UudF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.corr(numeric_only=True)['Burn Rate'][:]"
      ],
      "metadata": {
        "id": "WHIAKYyUWIlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(['Date of Joining','Days'],axis=1)\n",
        "#droping the column names Date of Joining and days from the data set or the dataframe"
      ],
      "metadata": {
        "id": "x10qQSqQVfvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "_KJ_SRC2Vrtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_columns  = data.select_dtypes('object').columns\n",
        "fig, ax = plt.subplots(nrows = 1, ncols =len(cat_columns),sharey=True, figsize=(10, 5))\n",
        "for i, c in enumerate(cat_columns):\n",
        "    sns.countplot(x=c, data=data, ax=ax[i])\n",
        "plt.show()\n",
        "\n",
        "#This code creates a series of count plots, one for each categorical column in the DataFrame `data`, arranged in a single row and\n",
        "#sharing the y-axis, to visualize the distribution of categories within each column."
      ],
      "metadata": {
        "id": "I7XNzwyZWSkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if the column exists before apllying get_dummies\n",
        "if all(col in data.columns for col in ['Company Type','WFH Setup Available','Gender']):\n",
        "  data = pd.get_dummies(data, columns=['Company Type','WFH Setup Available','Gender'],drop_first=True)\n",
        "  data.head()\n",
        "  encoded_columns = data.columns\n",
        "else:\n",
        "  print(\"ERROR : one or more columns not present in the DataFrame.\")\n",
        "  print(data.columns)\n",
        "\n",
        "#This code checks if the columns 'Company Type', 'WFH Setup Available', and 'Gender' exist in the DataFrame data, applies one-hot encoding\n",
        "#to these columns if they do, and prints the first few rows of the modified DataFrame and the names of the new columns, otherwise, it prints\n",
        "#an error message with the available columns."
      ],
      "metadata": {
        "id": "2TAzLm2LWxN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "A6wFhaKsXggn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spliti dataframe into x and y\n",
        "y=data['Burn Rate']\n",
        "x = data.drop('Burn Rate', axis=1)"
      ],
      "metadata": {
        "id": "BZ85eVL6Xj5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train-test split\n",
        "xtrain, xtest, ytrain,ytest = train_test_split(x,y,test_size=0.7,shuffle = True, random_state=1)\n",
        "\n",
        "#scale x\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(xtrain)\n",
        "xtrain = pd.DataFrame(scaler.transform(xtrain),index =xtrain.index, columns = xtrain.columns)\n",
        "xtest = pd.DataFrame(scaler.transform(xtest),index =xtest.index, columns = xtest.columns)\n",
        "\n",
        "#This code splits the data into training and testing sets with 70% of the data for testing, scales the features using `StandardScaler`\n",
        "#on the training set, and applies the same scaling to the testing set."
      ],
      "metadata": {
        "id": "G6HTvxPwXvIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_filename = '../models/scaler.pkl'\n",
        "os.makedirs(os.path.dirname(scaler_filename), exist_ok=True)\n",
        "with open(scaler_filename, 'wb') as scaler_file:\n",
        "    pickle.dump(scaler,scaler_file)\n",
        "\n",
        "#This code creates the directory for the scaler file if it doesn't exist and then saves the `StandardScaler` object to a file named\n",
        "#'scaler.pkl' using pickle."
      ],
      "metadata": {
        "id": "VGegrQYCYe4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain"
      ],
      "metadata": {
        "id": "5Z3mjC3yY9HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain"
      ],
      "metadata": {
        "id": "Ufr8nzHuZHIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving processed data\n",
        "path = '../data/processed'\n",
        "os.makedirs(path, exist_ok=True)\n",
        "xtrain.to_csv(path+'xtrain_processes.csv',index=False)\n",
        "xtest.to_csv(path+'xtest_processes.csv',index=False)\n",
        "\n",
        "#This code creates the directory for processed data if it doesn't exist and then saves the xtrain and xtest DataFrames as CSV files\n",
        "#in that directory."
      ],
      "metadata": {
        "id": "IMvPLZVrZHvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Building"
      ],
      "metadata": {
        "id": "LuZsAv87Zzlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "nUnjODkaZ8Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regression_model = LinearRegression()\n",
        "linear_regression_model.fit(xtrain,ytrain)\n",
        "\n",
        "#This code initializes a `LinearRegression` model and trains it using the `xtrain` and `ytrain` data."
      ],
      "metadata": {
        "id": "YGt86rP7Z_ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear Regressioin Performance Model\")\n",
        "#Making peredictionns on the test model\n",
        "ypred = linear_regression_model.predict(xtest)\n",
        "\n",
        "#calculating the mean square error\n",
        "mse = mean_squared_error(ytest,ypred)\n",
        "print(\"Mean Squared Error : \",mse)\n",
        "\n",
        "#calculating the root mean square error\n",
        "rmse = mean_squared_error(ytest,ypred,squared=False)\n",
        "print(\"Root Mean Squared Error : \",rmse)\n",
        "\n",
        "#calculating the mean absolute error\n",
        "mae = mean_absolute_error(ytest,ypred)\n",
        "print(\"Mean Absolute Error : \",mae)\n",
        "\n",
        "#calculating the R-squared score\n",
        "r2 = r2_score(ytest,ypred)\n",
        "print(\"R-squared Score : \",r2)\n",
        "\n",
        "#This code evaluates the performance of a trained linear regression model by making predictions on the test set,\n",
        "#and then calculates and prints the Mean Squared Error, Root Mean Squared Error, Mean Absolute Error, and R-squared score."
      ],
      "metadata": {
        "id": "Q5lz8ZjjaMwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wV9hMBNXaPiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}